\documentclass[a4paper]{article}
\author{Chris Swetenham (s1149322)}
\title{IAML Assignment}
\date{November 22, 2011}

\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}

\maketitle
\clearpage

\section*{Part A}
\subection*{Question i}
Training from \t{train_faces.arff} with 5-fold CV
NaiveBayes: Accuracy 12\%
J48: Accuracy 37\%
SMO: Accuracy 46\%
\subsection*{Question ii}
AddCluster: EM using 10 clusters corresponding to the 10 subjects in the dataset.
In the histogram for the cluster attribute, we notice that there are two clusters which dominate, with 100 instances each. We were hoping to see 10 roughly equal clusters corresponding to the 10 subjects in the dataset. At this point, checking the histogram of the class labels, we notice that these are unevenly distributed, and further we have 400 instances rather than the 20*10 we were promised. The two dominating classes together sum to 200, the discrepancy between the expected and actual size of the dataset. Finally, taking the clusters and labelling them by class, we see that there is very little correspondance between the two attributes.
The dominating clusters are \i{cluster2} and \i{cluster6}. Taking the first three images from each cluster: [6,12,20] and [2,15,29]:
Comparing this to the first three images from \{cluster1} and \{cluster3}: [1,7,8], [13,17,19]:

We see that the two dominating clusters seem to be pure white and noisy dark images which are not relevant to our task.

\subsection*{Question iii}
Using the filter \t{RemoveWithAttribute}, instances in \i{cluster2} and \{cluster6} were removed. The resulting dataset has 20 instances of each class, as expected from the task description.

\subsection*{Question iv}
After removing the cluster attribute, with 5-fold CV:
NaiveBayes: Accuracy 52.5\%
J48: Accuracy 62\%
SMO: Accuracy 88\%

We can compare this performance with the results from part i) as baseline. After removing the erroneous instances, the accuracy of all three classifiers has greatly improved. Since the dataset was smaller, the time taken to train and evaluate the classfiers was also much shorter.

\subsection*{Question v}
Training SMO classifier from \t{train_faces_clean.arff} with \t{val_faces.arff} as test set.

SMO: Accuracy 92\%

This is comparable to the performance on the training data earlier.

Two example correctly classified instances: [32, 85]
Two example incorrectly classified instances: [15, 42]

The classifier seems to make mistakes on instances where the subject's eyes are entirely in shadow.

\section*{Part B}
\subsection*{Question i}
In the Naive Bayes model, each of the attributes are independently distributed given the class label. This is not a very good assumption for our data set, since nearby pixels will usually have similar values, and lighting conditions affect pixels in similar ways across large regions of the image.

For continuous attributes, Naive Bayes will build a gaussian distribution of the values for each pixel for each subject's images. This is a poor choice, because the many lighting conditions mean that the variation of any pixel within one class is much greater than the variation of that pixel between classes.

Discretization allows for more arbitrary distributions to be learned, for example we might have a bimodal distribution for a pixel with one mode when that pixel is in shadow and one when it is well lit. Choosing the number of bins is a parameterization problem; the more bins we allow, the closer fit to the data we allow, but the more parameters we have to learn. On one hand, we want to choose a small enough number of bins that we have some bins with multiple instances in them. On the other hand, too few bins will give us very little discriminative power.

NaiveBayes with 5-fold CV, discretized pixel values:
5 bins:  45\%
10 bins: 57\%
20 bins: 61.5\%
40 bins: 62\%
60 bins: 62.5\%
80 bins: 60.5\%
100 bins: 59\%

Out of the settings tested, using 60 bins for discretization gives the best performance. We see a moderate gain in performance over the original NaiveBayes result (62.5\% vs 52.5\%); this is about what we would expect, since we improved on the original gaussian model, but NaiveBayes remains less than ideal for the task at hand since its assumptions about the structure of the data do not hold well for our dataset.

\subsection*{Question ii}
\t{InfoGainAttributeVal} works by computing the information gain of each attribute, which is the difference between the entropy of the marginal class distribution and the entropy of the class distribution conditioned on that attribute. The attribute with the highest information gain tells us the most about the class, and one with 0 information gain is independent of the class.

After removing the attributes with 0 information gain, I was left with 353 attributes - I note that the provided \t{train_faces_clean_best.arff} has 350 attributes.

\subsection*{Question iii}
The selected pixels with highest information gain cluster around the areas of the eyes, the corner of the chin, and the mouth and nose, ignoring the cheeks and the nose bridge. This corresponds with the most distinguishing features of a human face.



A potential problem with the information gain criterion is that if we use it to select more than one feature at once, these features might be correlated, and so the second we select would tell us a lot by itself but very little if we already know the first.

\subsection*{Question iv}
NaiveBayes: 60\% (vs 52.5\%)
J48: 63.5\% (vs 62\%)
SMO: 91\% (vs 88\%)

We argued in part i) that the Naive Bayes algorithm has trouble with the original dataset due to the gaussian model being a poor fit to the data. Keeping only the attributes with the highest information gain should have removed a lot of attributes which were poor predictors and strongly correlated with each other, giving us better performance as observed.

The decision tree algorithm already works by selecting the attributes with the highest information gain, so as expected we see very little difference.

The SVM algorithm TODO

\section*{Part C}
\subsection*{Question i}

The eigenvalues in descending order are plotted in Figure~TODO. As we can see there is a sharp fall-off after the first few eigenvalues which contribute most of the variation in the dataset.

\subsection*{Question ii}

After using the PrincipalComponents attribute filter to retain 95\% of the variation in the dataset, we are left with only 32 attributes.

NaiveBayes: 81.5\%
J48: 61.5\%
SMO: 82.5\%

PCA gives us components which are actually independent and so suits Naive Bayes well, and this is reflected in the improved performance of Naive Bayes on this version of the dataset.

The decision tree algorithm tries to find binary splits in the values of each attribute to classify instances. After PCA, most of the variation occurs in very few attributes which doesn't leave the decision tree algorithm much to work with.

The SMO algorithm got worse because... TODO

\subsection*{Question iii}

The eigenfaces corresponding to the largest eigenvalues capture gross detail, with successively finer features as one proceeds to smaller eigenvalues.

The largest two eigenvalues are 0.618 and 0.303. The corresponding eigenfaces are an average face, corresponding to the variation in level of global illumination in the dataset, and a face which looks lit from the side, corresponding to the variation in left-right illumination in the images. Only when we get some number of eigenfaces in do we start seeing ones which distinguish between facial features as opposed to illumination; this makes sense since in the dataset the difference between images with different illumination directions is much more dramatic than the difference between two faces under the same illumination.

\subsection*{Question iv}
Around 10 components are required to produce a face which is clearly recognisable as the original. As we increase the number of components, the face moves away from the average of the underlying faces and gains specific details from the face we are reconstructing.

\subsection*{Question v}
We remove the four attributes with the largest eigenvalues.

NaiveBayes: 82.5\% (vs 81.5\%)

The accuracy has increased slightly. As stated before the greatest part of the variation in the dataset is in the lighting, which is not significant to our classification task; so the attributes we have left after removing the four with largest variation serve well for classifying the data.

\subsection*{Question vi}
We try running SMO retaining different numbers of the top eigenfaces.

SMO 20 eigenfaces: 73.5\%
SMO 40 eigenfaces: 90\%
SMO 80 eigenfaces: 85\%
SMO 120 eigenfaces: 78\%
SMO 160 eigenfaces: 63\%
SMO 200 eigenfaces: 62.5\%

We notice that there is a peak in performance around 40 attributes, after which performance falls off. This is because TODO.
% This may be because with more attributes, the input data is sparser; or because the SVM algorithm has as many degrees of freedom as the number of attributes in the training data, and so can overfit if given more attributes - but then why did it do so well on the dataset before PCA??

\section*{Part D}
\subsection*{Question i}

Results at 99% confidence (95% is identical), by dataset:
Dataset                       NaiveBayes |     J48     SMO
------------------------------------------------------------
train_faces_clean           (25)   53.60 |   61.90     88.00 v
train_faces_clean_best      (25)   63.60 |   64.30     90.10 v
train_faces_clean_pca       (25)   79.90 |   60.10 *   80.90  
------------------------------------------------------------
                                 (v/ /*) |   (0/2/1)   (2/1/0)

We see that for two datasets, train_faces_clean and train_faces_clean_best, SMO does significantly better than NaiveBayes with 99% confidence.

Results at 99% confidence, by algorithm:

Dataset                   train_faces_clean | train_faces_clean_best     train_faces_clean_pca
----------------------------------------------------------------------------------------------
NaiveBayes                     (25)   53.60 |                  63.60 v                   79.90 v
J48                            (25)   61.90 |                  64.30                     60.10  
SMO                            (25)   88.00 |                  90.10                     80.90  
----------------------------------------------------------------------------------------------
                                    (v/ /*) |                  (1/2/0)                   (1/2/0)

We see that for NaiveBayes, train_faces_clean_best and train_faces_clean_pca are both significantly better than train_faces_clean with 99% confidence. For J48, none of the datasets are significantly better than the others.

Based on this, I would select TODO for further experiments. % train_faces_clean_best?

\subsection*{Question ii}

Randomizing the dataset with default seed of 42.

Using RemovePercent, invertselection true, values of 100 to 5.

100\% of Data (200 instances): 92\%
80\% of Data (160 instances): 87\%
65\% of Data (130 instances): 86.5\% 
50\% of Data (100 instances): 84\%
35\% of Data (70 instances): 76.5\%
10\% of Data (20 instances): 40\%
5\% of Data (10 instances): 21.5\%

As we can see from the plot above, the performance is tailing off somewhat as we add more data, but the last 20% of the dataset does provide a reasonably improvement and so a larger dataset could still improve the performance of the classifier.

\subsection*{Question iii}
Naive Bayes with PCA: 83.5\%
EM seed default 100, 10 clusters.
Naive Bayes with PCA and EM clusters: 86.5\%

Adding cluster labels has slightly helped. Although they don't contain any data independent of the original dataset, NaiveBayes assumes all attributes are independent given the class labels, and so doesn't make use of all the information in the original dataset; by adding new features which are distinct from any of the original ones and which are useful in themselves for classification, we can improve the performance of the classifier.

\end{document}
