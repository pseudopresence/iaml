\documentclass[a4paper]{article}
\author{Chris Swetenham (s1149322)}
\title{IAML Assignment}
\date{November 22, 2011}

\usepackage{fullpage}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}

\maketitle
\clearpage

\section*{Part A}
\subection*{Question i}
Training from \t{train_faces.arff} with 5-fold CV
NaiveBayes: Accuracy 12\%
J48: Accuracy 37\%
SMO: Accuracy 46\%
\subsection*{Question ii}
AddCluster: EM using 10 clusters corresponding to the 10 subjects in the dataset.
In the histogram for the cluster attribute, we notice that there are two clusters which dominate, with 100 instances each. We were hoping to see 10 roughly equal clusters corresponding to the 10 subjects in the dataset. At this point, checking the histogram of the class labels, we notice that these are unevenly distributed, and further we have 400 instances rather than the 20*10 we were promised. The two dominating classes together sum to 200, the discrepancy between the expected and actual size of the dataset. Finally, taking the clusters and labelling them by class, we see that there is very little correspondance between the two attributes.
The dominating clusters are \i{cluster2} and \i{cluster6}. Taking the first three images from each cluster: [6,12,20] and [2,15,29]:
Comparing this to the first three images from \{cluster1} and \{cluster3}: [1,7,8], [13,17,19]:

We see that the two dominating clusters seem to be pure white and noisy dark images which are not relevant to our task.

\subsection*{Question iii}
Using the filter \t{RemoveWithAttribute}, instances in \i{cluster2} and \{cluster6} were removed. The resulting dataset has 20 instances of each class, as expected from the task description.

\subsection*{Question iv}
After removing the cluster attribute, with 5-fold CV:
NaiveBayes: Accuracy 52.5\%
J48: Accuracy 62\%
SMO: Accuracy 88\%

We can compare this performance with the results from part i) as baseline. After removing the erroneous instances, the accuracy of all three classifiers has greatly improved. Since the dataset was smaller, the time taken to train and evaluate the classfiers was also much shorter.

\subsection*{Question v}
Training SMO classifier from \t{train_faces_clean.arff} with \t{val_faces.arff} as test set.

SMO: Accuracy 92\%

This is comparable to the performance on the training data earlier.

Two example correctly classified instances: [32, 85]
Two example incorrectly classified instances: [15, 42]

The classifier seems to make mistakes on instances where the subject's eyes are entirely in shadow.

\section*{Part B}
\subsection*{Question i}
In the Naive Bayes model, each of the attributes are independently distributed given the class label. This is not a very good assumption for our data set, since nearby pixels will usually have similar values, and lighting conditions affect pixels in similar ways across large regions of the image.

For our dataset, Naive Bayes will build a distribution of the values for each pixel for each subject's images. Each subject only has 20 images and there are 255 possible pixel values, so this will be a very sparse distribution; and with or without +1 smoothing applied, this will make for poor predictions.

On one hand, we want to choose a small enough number of bins that we have some bins with multiple instances in them. On the other hand, too few bins will give us very little discriminative power. 

NaiveBayes with 5-fold CV, discretized pixel values:
5 bins:  45\%
10 bins: 57\%
20 bins: 61.5\%
40 bins: 62\%
60 bins: 62.5\%
80 bins: 60.5\%
100 bins: 59\%

Out of the settings tested, using 60 bins for discretization gives the best performance. We see a moderate gain in performance over the original NaiveBayes result (62.5\% vs 52.5\%); this is about what we would expect, since we improved on the original sparse dataset but NaiveBayes remains less than ideal for the task at hand since its assumptions about the structure of the data do not hold well for our dataset.

\subsection*{Question ii}
\t{InfoGainAttributeVal} works by computing the information gain of each attribute, which is the difference between the entropy of the marginal class distribution and the entropy of the class distribution conditioned on that attribute. The attribute with the highest information gain tells us the most about the class, and one with 0 information gain is independent of the class.

After removing the attributes with 0 information gain, I was left with 353 attributes - I note that the provided \t{train_faces_clean_best.arff} has 350 attributes.

\subsection*{Question iii}
The selected pixels with highest information gain cluster around the areas of the eyes, the corner of the chin, and the mouth and nose, ignoring the cheeks and the nose bridge. This corresponds with the most distinguishing features of a human face.



A potential problem with the information gain criterion is that if we use it to select more than one feature at once, these features might be correlated, and so the second we select would tell us a lot by itself but very little if we already know the first.

\subsection*{Question iv}
NaiveBayes: 60\% (vs 52.5\%)
SMO: 91\% (vs 88\%)

We showed in part i) that the Naive Bayes algorithm has trouble with the original dataset due to the sparsity of values. If we had a much denser dataset, attributes which are poor predictors of class label would not affect classification performance, but since we do not they are actively harmful and removing them slightly improves our performance.

The SVM algorithm can and does simply assign near-0 weights to attributes which don't contribute much; for example \t{pixel_813} was given a weight of 0.001 


*** TODO just noticed that NaiveBayes models each probability with a gaussian rather than a discrete distribution when we haven't performed discretization, need to redo sections on NaiveBayes above :(

\end{document}
